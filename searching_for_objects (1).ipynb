{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99qU0IDt8u9A"
      },
      "source": [
        "Midsemester Group 8\n",
        "\n",
        "Members:\n",
        "Kirk Kudoto\n",
        "Welile N. Dlamini\n",
        "\n",
        "21st July 2024\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuS3F0Hm8QKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e1d45d-850b-4888-891f-5f43cf583eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egJD9BHL8Rfm"
      },
      "outputs": [],
      "source": [
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib"
      ],
      "metadata": {
        "id": "QjhWFdNTJhne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3vj3Y-vkV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9605eb68-4ce2-4a5a-899d-9a1586447dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdmk1y7N493I"
      },
      "source": [
        "Splitting a video into frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBWuSL2D3V9w"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def split_video_to_frames(video_path, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    if os.path.exists(output_folder):\n",
        "        shutil.rmtree(output_folder)\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not retrieved, then we have reached the end of the video\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save the frame as an image\n",
        "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # When everything done, release the video capture object\n",
        "    cap.release()\n",
        "    print(f\"Extracted {frame_count} frames to {output_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75pUlWhlA2OG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc0eeae-c92f-4a3b-eeca-bf3a8b0d167f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What are you looking for?soccer ball\n"
          ]
        }
      ],
      "source": [
        "find = input(\"What are you looking for?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r19YnBlDFvUL"
      },
      "outputs": [],
      "source": [
        "def finder(list, x):\n",
        "  count = 0\n",
        "\n",
        "  for i in list:\n",
        "    if x in i:\n",
        "      return count\n",
        "    else:\n",
        "      count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzkstYNGCQTL"
      },
      "source": [
        "Trying work on individual frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FO67jWncYu7"
      },
      "outputs": [],
      "source": [
        "def get_predictions(image_path):\n",
        "  img = Image.open(image_path)\n",
        "\n",
        "  from prettytable import PrettyTable\n",
        "  table = PrettyTable([\"Categories\", \"Probability\"])\n",
        "\n",
        "  tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(tensor)\n",
        "  probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "  print(probabilities.shape)\n",
        "\n",
        "  # Get imagenet class mappings\n",
        "  url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n",
        "  urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "  with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "      categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "  # Print top categories per image\n",
        "  top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "  for i in range(top5_prob.size(0)):\n",
        "      table.add_row([categories[top5_catid[i]], str(round(top5_prob[i].item()*100, 2))+\"%\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZtmBV3DKzkS"
      },
      "source": [
        "Allowing the user to search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "ocAYWFb7C3g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MaXUbusK4ON"
      },
      "outputs": [],
      "source": [
        "# Function to search for an object in frames\n",
        "def search_object_in_frames(folder, search_object):\n",
        "    object_found = False\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        img = Image.open(image_path)\n",
        "        tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(tensor)\n",
        "        probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "\n",
        "        # Find the index of the search object in categories\n",
        "        try:\n",
        "            object_idx = categories.index(search_object)\n",
        "        except ValueError:\n",
        "            return f\"Error: '{search_object}' is not a recognizable object in ImageNet classes.\"\n",
        "\n",
        "        # Get the probability of the object being in the frame\n",
        "        object_probability = probabilities[object_idx]\n",
        "\n",
        "        # Check if the probability is above a threshold\n",
        "        if object_probability.item() > 0.9:\n",
        "            print(f\"Object found in {filename} with probability {object_probability.item() * 100:.2f}%\")\n",
        "            object_found = True\n",
        "            img.show()\n",
        "\n",
        "    if not object_found:\n",
        "        print(\"Object doesn't exist!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI55xIB2mBGO"
      },
      "outputs": [],
      "source": [
        "def get_highest_prob(file_probs):\n",
        "  highest_prob = file_probs[0][-1]\n",
        "  highest_file = file_probs[0][0]\n",
        "  highest_index = 0\n",
        "\n",
        "  for ind, i in enumerate(file_probs):\n",
        "    if i[-1] > highest_prob:\n",
        "      highest_prob = i[-1]\n",
        "      highest_file = i[0]\n",
        "      highest_index = ind\n",
        "  return highest_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nREga7YLMcT"
      },
      "outputs": [],
      "source": [
        "# Function to search for an object in frames\n",
        "def search_object_in_frames(folder, search_object):\n",
        "    searches = list()\n",
        "    object_found = False\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        img = Image.open(image_path)\n",
        "        tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(tensor)\n",
        "        probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "\n",
        "        # Find the index of the search object in categories\n",
        "        try:\n",
        "            object_idx = categories.index(search_object)\n",
        "        except ValueError:\n",
        "            return f\"Error: '{search_object}' is not a recognizable object in ImageNet classes.\"\n",
        "\n",
        "        # Get the probability of the object being in the frame\n",
        "        object_probability = probabilities[object_idx]\n",
        "\n",
        "        # Check if the probability is above a threshold\n",
        "        if object_probability.item() > 0.1:\n",
        "            next = [filename, object_probability.item() * 100]\n",
        "            searches.append(next)\n",
        "            object_found = True\n",
        "\n",
        "    for i in searches:\n",
        "      img = Image.open(os.path.join(folder, i[0]))\n",
        "      img.show()\n",
        "\n",
        "    if not object_found:\n",
        "        print(\"Object doesn't exist!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==7.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ws4fxvhHJTr",
        "outputId": "2d737bd9-3152-4150-95a8-282dfd0875c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok==7.2.0 in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok==7.2.0) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VrBDqCbzrGA",
        "outputId": "0d906c02-5157-4aaa-bb25-2044291caa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2.0)\n",
            "Requirement already satisfied: blinker in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.33.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=3.2.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->streamlit) (3.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.21.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.21.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->streamlit) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import pandas as pd\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load and return the pretrained Inception V3 model.\"\"\"\n",
        "    model = timm.create_model('inception_v3', pretrained=True)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def transform_image(image):\n",
        "    \"\"\"Apply transformations to the image and return a tensor.\"\"\"\n",
        "    from timm.data import resolve_data_config\n",
        "    from timm.data.transforms_factory import create_transform\n",
        "    config = resolve_data_config({}, model = model)\n",
        "    transform = create_transform(**config)\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "def predict(image, model):\n",
        "    \"\"\"Predict the class probabilities of the image with the model.\"\"\"\n",
        "    tensor = transform_image(image)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tensor)\n",
        "    return torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def split_video_to_frames(video_path, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    if os.path.exists(output_folder):\n",
        "        shutil.rmtree(output_folder)\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not retrieved, then we have reached the end of the video\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save the frame as an image\n",
        "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # When everything done, release the video capture object\n",
        "    cap.release()\n",
        "    print(f\"Extracted {frame_count} frames to {output_folder}\")\n",
        "\n",
        "def search_object_in_frames(folder, search_object):\n",
        "    object_found = False\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        img = Image.open(image_path)\n",
        "        tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(tensor)\n",
        "        probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "\n",
        "        # Find the index of the search object in categories\n",
        "        try:\n",
        "            object_idx = categories.index(search_object)\n",
        "        except ValueError:\n",
        "            return f\"Error: '{search_object}' is not a recognizable object in ImageNet classes.\"\n",
        "\n",
        "        # Get the probability of the object being in the frame\n",
        "        object_probability = probabilities[object_idx]\n",
        "\n",
        "        # Check if the probability is above a threshold\n",
        "        if object_probability.item() > 0.9:\n",
        "            print(f\"Object found in {filename} with probability {object_probability.item() * 100:.2f}%\")\n",
        "            object_found = True\n",
        "            img.show()\n",
        "\n",
        "    if not object_found:\n",
        "        print(\"Object doesn't exist!!!\")\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Classifier Using InceptionV3\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a video...\", type=[\"mov\", \"avi\", \"mp4\"])\n",
        "\n",
        "    search_element = st.text_input(\"What are you looking for?\")\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        split_video_to_frames(uploaded_file, '/content/drive/MyDrive/GoogleInceptionModel/current_frames')\n",
        "        search_object_in_frames('/content/drive/MyDrive/GoogleInceptionModel/current_frames', search_element)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VzniawBI1B",
        "outputId": "1fe402fa-5e99-44a1-97f6-7c6ff79eebf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export STREAMLIT_SERVER_MAX_UPLOAD_SIZE=40"
      ],
      "metadata": {
        "id": "JbnEZEf6lGee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def check_config():\n",
        "    st.write(\"Current max upload size:\", st.config.get_option(\"server.maxUploadSize\"))\n",
        "\n",
        "\n",
        "check_config()"
      ],
      "metadata": {
        "id": "hp9briG4e_OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "MAX_FILE_SIZE = 40\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load and return the pretrained Inception V3 model.\"\"\"\n",
        "    model = timm.create_model('inception_v3', pretrained=True)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "\n",
        "def transform_image(image, model):\n",
        "    \"\"\"Apply transformations to the image and return a tensor.\"\"\"\n",
        "    from timm.data import resolve_data_config\n",
        "    from timm.data.transforms_factory import create_transform\n",
        "    config = resolve_data_config({}, model=model)\n",
        "    transform = create_transform(**config)\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "def split_video_to_frames(video_file, output_folder):\n",
        "    # Save the uploaded video file to a local path\n",
        "    video_path = '/tmp/temp_video.mp4'\n",
        "    with open(video_path, 'wb') as file:\n",
        "        file.write(video_file.read())\n",
        "\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if os.path.exists(output_folder):\n",
        "        try:\n",
        "            shutil.rmtree(output_folder)\n",
        "            time.sleep(1)  # Pause to ensure the deletion completes\n",
        "        except OSError as e:\n",
        "            print(f\"Error: Could not completely clear the folder {output_folder}: {e}\")\n",
        "\n",
        "\n",
        "    # Ensure the folder is created\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if video opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not retrieved, then we have reached the end of the video\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save the frame as an image\n",
        "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # When everything done, release the video capture object\n",
        "    cap.release()\n",
        "    print(f\"Extracted {frame_count} frames to {output_folder}\")\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "def main():\n",
        "\n",
        "    st.title(\"Video Analysis and Object Finding Using Inception V3\")\n",
        "    st.write(\"Current max upload size:\", st.config.get_option(\"server.maxUploadSize\"))\n",
        "    uploaded_file = st.file_uploader(\"Upload a video...\", type=[\"mov\", \"avi\", \"mp4\"])\n",
        "    search_element = st.text_input(\"What are you looking for?\").lower()\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Notify the user that the upload was successful and processing will start\n",
        "        st.success(\"Upload successful! Processing the video...\")\n",
        "\n",
        "        # Create a temporary directory to store the video frames\n",
        "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "            # Handle the video upload and frame splitting\n",
        "            try:\n",
        "                split_video_to_frames(uploaded_file, tmpdirname)\n",
        "                st.success(\"Video has been split into frames.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to split video into frames: {e}\")\n",
        "\n",
        "            # Perform object search if a search term is provided\n",
        "            if search_element:\n",
        "                try:\n",
        "                    result = search_object_in_frames(tmpdirname, search_element)\n",
        "                    if result:\n",
        "                        st.success(f\"Found '{search_element}' in the video.\")\n",
        "                    else:\n",
        "                        st.warning(f\"Could not find '{search_element}' in the video.\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Failed to search for object: {e}\")\n",
        "            else:\n",
        "                st.warning(\"Please enter an object to search for in the video frames.\")\n",
        "\n",
        "        if st.button(\"Click to show instructions\"):\n",
        "            st.write(\"\"\"\n",
        "                Please upload a video file (MOV, AVI, MP4) and enter an object name you wish to search for.\n",
        "                The system will process the video, split it into frames, and look for the specified object across the frames.\n",
        "            \"\"\")\n",
        "\n",
        "def split_video_to_frames(video_file, output_folder):\n",
        "    # Write the uploaded video file to a temporary file\n",
        "    video_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n",
        "    with open(video_path, 'wb') as file:\n",
        "        file.write(video_file.read())\n",
        "\n",
        "    # Ensure the output directory is clean\n",
        "    if os.path.exists(output_folder):\n",
        "        shutil.rmtree(output_folder)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Process the video file\n",
        "    import cv2\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        st.error(\"Failed to open video file.\")\n",
        "        return\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    st.info(f\"Extracted {frame_count} frames to {output_folder}\")\n",
        "\n",
        "def search_object_in_frames(folder, search_object):\n",
        "    from PIL import Image\n",
        "    import torch\n",
        "\n",
        "    model = load_model()  # Make sure to define load_model or import it if it's not in this script\n",
        "    object_found = False\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        image_path = os.path.join(folder, filename)\n",
        "        img = Image.open(image_path)\n",
        "        tensor = transform_image(img, model)  # Ensure transform_image is defined or imported\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tensor)\n",
        "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "        try:\n",
        "            object_idx = categories.index(search_object)  # 'categories' should be defined elsewhere\n",
        "        except ValueError:\n",
        "            st.error(f\"'{search_object}' is not a recognizable object in ImageNet classes.\")\n",
        "            return False\n",
        "\n",
        "        object_probability = probabilities[object_idx]\n",
        "        if object_probability.item() > 0.3:  # Adjusting the threshold to 0.3 as necessary\n",
        "            object_found = True\n",
        "            st.image(img, channels = \"BGR\", caption = f\"Object found in {filename} with probability {object_probability.item() * 100:.2f}%\")\n",
        "\n",
        "    if not object_found:\n",
        "        st.warning(\"Object does not exist!!!\")\n",
        "    return object_found\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdBAspodQPwA",
        "outputId": "a9ab1866-31d4-4a3f-b96a-b08a6c6445b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import signal\n",
        "\n",
        "# List all running processes and find the ngrok process\n",
        "!ps -aux | grep ngrok\n",
        "\n",
        "\n",
        "# Assuming the ngrok process ID (PID) is at index 1 of the output\n",
        "output = !ps -aux | grep ngrok\n",
        "for line in output:\n",
        "    if 'ngrok' in line and 'grep' not in line:\n",
        "        pid = int(line.split()[1])\n",
        "        # Kill the ngrok process\n",
        "        os.kill(pid, signal.SIGTERM)\n",
        "        print(f\"ngrok process with PID {pid} has been terminated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7j2rgQKsM2",
        "outputId": "7010526b-f110-44af-904a-66179a8f273d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        5498  0.0  0.0   7376  3416 ?        S    23:35   0:00 /bin/bash -c ps -aux | grep ngrok\n",
            "root        5500  0.0  0.0   7376   280 ?        R    23:35   0:00 /bin/bash -c ps -aux | grep ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "import signal\n",
        "\n",
        "# Function to kill any existing ngrok processes\n",
        "def kill_ngrok():\n",
        "    ngrok.kill()\n",
        "\n",
        "# Set the ngrok auth token\n",
        "ngrok.set_auth_token(\"2jZqPXDqGHUARxWpfOS6ZymJiY8_4Udqg3peujBz4rSheHLMZ\")\n",
        "\n",
        "# Kill existing ngrok processes\n",
        "kill_ngrok()\n",
        "\n",
        "# Start the Streamlit app in a subprocess\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "# Connect to the ngrok tunnel on a potentially new port\n",
        "try:\n",
        "    public_url = ngrok.connect(addr='8501', proto='http')\n",
        "    print(f'Streamlit at {public_url}')\n",
        "except Exception as e:\n",
        "    print(\"Failed to create ngrok tunnel:\", e)\n",
        "    kill_ngrok()\n",
        "    # Attempt to reconnect\n",
        "    public_url = ngrok.connect(addr='8501', proto='http')\n",
        "    print(f'Streamlit at {public_url}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7ZLb3iCMxM9",
        "outputId": "d7352dce-47a3-4333-e8e7-867973f3311c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Updating authtoken for default \"config_path\" of \"ngrok_path\": /root/.config/ngrok/ngrok\n",
            "2024-07-21 23:36:11.355 Updating authtoken for default \"config_path\" of \"ngrok_path\": /root/.config/ngrok/ngrok\n",
            "INFO:pyngrok.ngrok:Opening tunnel named: http-8501-21620506-58d7-4ebe-be53-c1846529ae7d\n",
            "2024-07-21 23:36:11.419 Opening tunnel named: http-8501-21620506-58d7-4ebe-be53-c1846529ae7d\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2024-07-21 23:36:11.493 t=2024-07-21T23:36:11+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "2024-07-21 23:36:11.497 t=2024-07-21T23:36:11+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "2024-07-21 23:36:11.500 t=2024-07-21T23:36:11+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2024-07-21 23:36:11.595 t=2024-07-21T23:36:11+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "2024-07-21 23:36:11.839 t=2024-07-21T23:36:11+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2024-07-21 23:36:11.843 t=2024-07-21T23:36:11+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=76ab625ec536b63b\n",
            "2024-07-21 23:36:11.859 t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=76ab625ec536b63b\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=76ab625ec536b63b status=200 dur=349.331µs\n",
            "2024-07-21 23:36:11.868 t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=76ab625ec536b63b status=200 dur=349.331µs\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=ebf439ed89efb86e\n",
            "2024-07-21 23:36:11.876 t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=ebf439ed89efb86e\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=ebf439ed89efb86e status=200 dur=175.713µs\n",
            "2024-07-21 23:36:11.882 t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=ebf439ed89efb86e status=200 dur=175.713µs\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=340f8ffd54b6da50\n",
            "2024-07-21 23:36:11.887 t=2024-07-21T23:36:11+0000 lvl=info msg=start pg=/api/tunnels id=340f8ffd54b6da50\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-21620506-58d7-4ebe-be53-c1846529ae7d addr=http://localhost:8501 url=https://d0f8-34-125-237-21.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit at NgrokTunnel: \"https://d0f8-34-125-237-21.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-21 23:36:11.930 t=2024-07-21T23:36:11+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8501-21620506-58d7-4ebe-be53-c1846529ae7d addr=http://localhost:8501 url=https://d0f8-34-125-237-21.ngrok-free.app\n",
            "INFO:pyngrok.process.ngrok:t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=340f8ffd54b6da50 status=201 dur=71.517522ms\n",
            "2024-07-21 23:36:11.933 t=2024-07-21T23:36:11+0000 lvl=info msg=end pg=/api/tunnels id=340f8ffd54b6da50 status=201 dur=71.517522ms\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}